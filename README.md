# ğŸ“Š UMD Softball Performance Analytics ETL Pipeline  

## ğŸš€ Project Overview  
This project builds an **ETL pipeline** to extract, transform, and load **25 years of UMD Softball performance data** into **Snowflake** for analytics and reporting. The objective is to ensure **data accuracy, automation, and efficient querying**.  

## ğŸ—ï¸ Key Features  
âœ” **Data Extraction & Transformation:** Processed structured and unstructured data from historical records.  
âœ” **Data Pipeline Development:** Automated ingestion via **AWS S3** and **Apache Airflow**.  
âœ” **Data Cleaning & Processing:** Standardized formats, removed inconsistencies, and enriched datasets.  
âœ” **Snowflake Integration:** Efficient loading into **Snowflake for scalable analysis**.  

---

## ğŸ“‚ Repository Structure  
```
ğŸ“ umd-softball-etl
â”‚â”€â”€ ğŸ“‚ data/ # Raw and processed datasets
â”‚â”€â”€ ğŸ“‚ scripts/ # Python scripts for data processing & pipeline automation
â”‚â”€â”€ ğŸ“‚ dags/ # Apache Airflow DAGs for orchestration
â”‚â”€â”€ ğŸ“‚ notebooks/ # Jupyter notebooks for ETL development and testing
â”‚â”€â”€ ğŸ“„ README.md # Project documentation
â”‚â”€â”€ ğŸ“„ requirements.txt # List of required dependencies
```  

> **ğŸ“¢ Note:** Due to GitHubâ€™s file size limitations, **large datasets are stored externally in AWS S3**. Contact the repo owner for access.  

---

## ğŸ› ï¸ Tech Stack  
- **Languages:** Python  
- **Libraries:** `pandas`, `boto3`, `snowflake-connector-python`, `airflow`, `pyarrow`  
- **Data Storage:** AWS S3  
- **Workflow Orchestration:** Apache Airflow  
- **Database:** Snowflake  

---

## ğŸ“Š Results  
âœ… **Automated ETL pipeline** with **end-to-end data flow**  
âœ… **Integrated AWS S3 & Snowflake** for seamless storage and querying  
âœ… **Scalable & efficient Airflow DAGs** for job orchestration  

---

## ğŸ“Œ How to Run the Project  
1. Clone the repository:  
   ```bash
   git clone https://github.com/yourusername/umd-softball-etl.git
   ```  

2. Install dependencies:  
   ```bash
   pip install -r requirements.txt
   ```  

3. Set up AWS credentials for S3 access.  

4. Execute the pipeline:  
   - Run `scripts/convert_to_csv.py` to preprocess data.  
   - Run `scripts/upload_to_s3.py` to upload files to AWS S3.  
   - Run `scripts/etl_pipeline.py` to clean and transform data.  
   - Trigger **Apache Airflow DAGs** from `dags/softball_etl_dag.py`.  

---
## ğŸ“¢ Future Improvements  
ğŸ”¹ Implement **real-time data streaming** for continuous updates  
ğŸ”¹ Optimize performance using **parallel processing & caching**  
ğŸ”¹ Develop a **dashboard** for performance analytics in Snowflake  
ğŸ”¹ Expand Airflow DAGs for incremental data loads  

---
## ğŸ“¬ Contact  
For any questions, feel free to reach out via [LinkedIn](https://www.linkedin.com/in/tanmaysakharkar) or open an issue!  
